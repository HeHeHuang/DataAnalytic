# Decision Tree 

决策树分类算法
if else


代码中的if-else是由程序员编写的，而决策树的if-else需要通过数据集学习，然后自行生成。如何选择判断条件来生成判断分支是决策树算法的核心要点，有人称之为节点划分，也有人称之为节点分裂，指的都是生成if-else分支的过程。


基本原理都是用一长串的if-else完成样本分类，区别主要在纯度度量等细节上选择了不同的解决方案。

判别条件从何而来呢？

Attribute: 
? column name 

怎样才算是好的决策条件
 - 纯度
 - 决策树分类算法的不同 体现在怎样衡量纯度上
 - 节点纯度
 

纯度的度量方法:知道任何一项指标都需要量化度量方法
  需要满足的要求
  - 当一个分支下的所有样本都属于同一个类时，纯度达到最高值。
  - 当一个分支下样本所属的类别一半是正类一半是负类时，纯度取得最低值。
  - 纯度考察的是同一个类的占比，并不在乎该类究竟是正类还是负类，譬如某个分支下无论是正类占70%，还是负类占70%，纯度的度量值都是一样的。

剪枝算法:过拟合是决策树分类算法容易出现的问题，这个问题会影响决策树算法分类的有效性


**缺点**过于细节:各个维度的重要性扩充if-else分支,知道满足条件.
实际无效的属性维度就会被决策树算法当作有效的分支判断条件。用这种存在假性关联数据集训练得到的决策树模型就会出现过度学习的情况，学到了并不具备普遍意义的分类决策条件，也就是出现过拟合，导致决策树模型的分类有效性降低。

**怎么解决**
- 预剪枝
- 后剪枝



如果剪枝后决策树模型的分类在验证集上的有效性能够得到提高，就判定为需要进行剪枝，否则不剪枝。请注意，这里剪枝所使用的数据集不再是训练模型所使用的训练集，而是选择使用验证集来进行相关判断。

?有什么例子么? 


ID3、C4.5和CART
信息增益、增益率和基尼指数这三种不同的指标作为决策条件的选择依据。

#### 决策树分类算法的数学解析

##### 1.信息熵

#####  2．基尼指数

##### 3. 

### caterigrical  

### value Decision Tree

### 建立model


### 评估model


```python

```


```python

```


```python

```


```python

```


```python

```
